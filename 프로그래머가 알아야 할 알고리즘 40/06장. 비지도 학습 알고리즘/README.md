# 06장. 비지도 학습 알고리즘

# 1. 비지도 학습 이해하기

- 데이터에 내재된 패턴을 탐색하고 이를 이용해 비정형 데이터를 구조화하는 프로세스

## 1. 데이터 마이닝 사이클에서의 비지도 학습

- CRISP-DM(Cross-Industry Standard Process for Data Mining) 라이프 사이클
    - 비즈니스 이해 : 비즈니스 관점에서 문제와 요구사항을 세밀하게 이해
    - 데이터 이해 : 데이터 마이닝에 사용할 데이터 이해
    - 데이터 준비 : 훈련할 머신러닝 모델에 필요한 데이터 준비
    - 모델링 : 앞서 발견한 패턴을 활용해서 지도 학습 수행
    - 평가 : 훈련이 끝난 모델의 성능을 3단계에서 만든 테스트 데이터로 평가
    - 배포 : 모델을 프로덕션 환경에 배포하여 1단계에서 정의한 문제의 솔루션 제공
- SEMMA(Sample, Explore, Modify, Model, Access) 데이터 마이닝 프로세스

## 2. 비지도 학습의 최신 연구 트렌드

- 지도하지 않기 때문에 가정에 덜 의존적
- 어떤 차원에서든 잠재적으로 수렴할 수 있음

## 3. 비지도 학습의 활용 사례

- 마케팅 세분화
- 사기 탐지
- 장바구니 분석
- 음성 분류
- 문서 분류

# 2. 클러스터링 알고리즘 이해하기

## 1. 유사도 측정하기

### 1. 유클리드 거리(Euclidean)

- 다차원 공간에 위치한 두 포인트 사이의 가장 짧은 거리

### 2. 맨해튼 거리(Manhatten)

- 두 포인트 사이의 가장 긴 경로
- 맨해튼 거리 ≥ 유클리드 거리

### 3. 코사인 거리

- 원점에 연결된 두 포인트가 만들어내는 각도의 코사인 값을 계산하여 구함

### 4. k-평균 클러스터링 알고리즘

- 평균을 이용해 데이터 포인트 간 거리 계산, k개의 클러스터 생성
- 클러스터의 중심점이 클러스터에 속하는 데이터 포인트를 가장 잘 대표할 때까지 중심점을 옮기는 과정 반복

### 5. k-평균 클러스터링 알고리즘 로직

- 초기 설정 : 거리 측정 방식 결정
- 클러스터 개수 k를 정함
- 데이터 포인트 중에서 k개를 골라 클러스터 중심점을 설정
- 선택한 거리 측정 방식을 이용해 문제 공간상의 각 데이터 포인트와 k개의 클러스터 중심점 사이의 거리를 반복적으로 계산
- 문제 공간상의 각 데이터 포인트를 가장 가까운 클러스터 중심점에 할당
- 현재 설정된 클러스터 중심점이 실제로 각 클러스터의 무게 중심인지 확인
- 종료 조건 : 클러스터 중심점이 더 이상 변화하지 않을 때
    - 최대 실행 시간 설정
    - 최대 반복 횟수 설정

### 6. k-평균 클러스터링 알고리즘 코딩하기

```python
from sklearn import cluster
import pandas as pd
import numpy as np
import matplotlib.plot as plt

dataset = pd.DataFrame({'x': [11, 21, 28, 17, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 62, 70, 72, 10],
'y':[39, 36, 30, 52, 53, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 18, 7, 24, 10]})

myKmeans = cluster.KMeans(n_clusters=2)
myKmeans.fit(dataset)

labels = myKmeans.labels_
centers = myKmeans.cluster_centers_

print(labels)
print(centers)

plt.scatter(dataset['x'], dataset['y'], s=10)
plt.scatter(centers[0], centers[1], s=100)
plt.show()
```

### 7. k-평균 클러스터링 알고리즘의 한계

- 클러스터 개수를 미리 설정해야 함
- 클러스터 중심점을 초기에 무작위로 설정 → 알고리즘을 실행할 때마다 클러스터링 결과 달라짐
- 각 데이터 포인트를 오직 하나의 클러스터에만 할당
- 이상치에 취약

## 2. 계층적 클러스터링 알고리즘

- 상향(Bottom-Up) 방식 알고리즘
- 비슷한 데이터 포인트끼리 묶어서 점진적으로 클러스터 중심점으로 이동

### 1. 계층적 클러스터링 알고리즘의 단계

- 문제 공간에 있는 각 데이터 포인트마다 클러스터 생성
- 서로 가장 가까이 위치한 포인트끼리 묶음
- 종료 조건 확인
- 덴드로그램(Dendrogram) : 알고리즘을 통해 얻는 클러스터 구조

### 2. 계층적 클러스터링 알고리즘 코딩하기

```python
from sklean.cluster import AgglomerativeClustering
import pandas as pd
import numpy as np

dataset = pd.DataFrame({'x': [11, 21, 28, 17, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 62, 70, 72, 10],
'y':[39, 36, 30, 52, 53, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 18, 7, 24, 10]})

cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
cluster.fit_predict(dataset)

print(cluster.labels_)
```

## 3. 클러스터 평가하기

- 같은 클러스터에 할당된 데이터 포인트들은 가능한 서로 비슷해야 함
- 다른 클러스터에 할당된 데이터 포인트들은 가능한 서로 달라야 함
- 실루엣 분석(Silhouette Analysis) : 클러스터가 얼마나 뭉쳐 있고 분리되어 있는지 평가하는 기법
    - 0.71 ~ 1.00 : 상, 클러스터들이 서로 상당히 분리되어 있음
    - 0.51 ~ 0.70 : 중 : 클러스터들이 서로 어느 정도 분리되어 있음
    - 0.26 ~ 0.50 : 하, 클러스터가 만들어지기는 했으나 그 결과를 신뢰하기 어려움
    - < 0.25 : 클러스터가 발견되지 않음, 입력한 파라미터와 데이터를 이용해 클러스터링하는 데 실패

## 4. 클러스터링 활용 사례

- 범죄 다발 지역 분석
- 인구 통계 및 사회 분석
- 시장 세분화
- 타게팅 광고
- 고객 카테고리 분류
- 주성분 분석(Principal Component Analysis, PCA)

# 3. 차원 축소 알고리즘 이해하기

- 차원 축소(Dimensionally Reduction) : 특성의 개수를 최소화하면 문제를 단순하게 만들 수 있음
- 특성 선별(Feature Selection) : 우리가 풀려는 문제의 맥락에 맞는 중요한 특성들만 선택
- 특성 조합(Feature Aggregation) : 다음 알고리즘 중 하나를 이용해 둘 이상의 특성을 조합하여 차원을 축소함
  - 주성분 분석(PCA) : 선형 비지도 학습 알고리즘
  - 선형 판별 분석(Linear Discriminant Analysis, LDA) : 선형 지도 학습 알고리즘
  - 커널 주성분 분석(Kernel PCA) : 비선형 알고리즘

## 1. 주성분 분석

- 선형 결합을 이용해 차원을 축소하는 비지도 학습 기법

    ```python
    from sklearn.decomposition import PCA
    
    iris = pd.read_csv('iris.csv')
    
    X = iris.drop('Species', axis=1)
    pca = PCA(n_compnents=4)
    pca.fit(X)
    
    pca_df = pd.DataFrame(pca.components_, columns=X.columns)
    
    print(pca.explained_variance_ratio)
    ```


## 2. 주성분 분석의 한계

- 연속형 변수만 처리 가능 → 카테고리형 변수 부적합
- 특성을 결합할 때 값을 근사하여 처리
- 변수들이 상관 관계를 형성한다는 가정 사용
- 선형적인 변수나 변수 간 관계에 적합

# 4. 연관 규칙 마이닝(Association Rules Mining) 이해하기

- 패턴의 빈도 측정 가능
- 패턴들이 형성하는 인과 관계 분석 가능
- 무작위 추측 대비 정확도를 계산하여 패턴의 유용성 정량화 가능

## 1. 연관 규칙 마이닝의 활용 사례

- 여러 변수들이 가진 인과 관계 조사 시 유용

## 2. 장바구니 분석(Basket Analysis)

- 추천 엔진의 간단한 버전
- 아이템들이 어떻게 서로 연결되어 있는지 분석 가능

## 3. 연관 규칙

- 거래 데이터에 담긴 아이템들이 형성하는 관계를 수학적으로 표현
- 사소한 규칙 : 생성된 규칙 중 대다수는 아주 상식적이어서 쓸모가 없음
- 해석 불가능한 규칙 : 전혀 연관성이 없는 두 이벤트 사이에 의미 없는 관계 표현
- 실행 가능한 규칙 : 연관 규칙 마이닝으로 얻으려는 규칙

## 4. 평가 척도

- 지지도 : 해당 패턴이 데이터셋에서 얼마나 자주 등장하는지 나타냄
- 신뢰도 : 조건부 확률을 이용해 왼편과 오른편을 얼마나 강하게 연관 지을 수 있는지 정량화
- 향상도 : 무작위로 예측하는 것에 비해 주어진 조건을 이용한 예측의 개선 효과가 얼마나 더 큰지 표현

## 5. 연관 규칙 마이닝 알고리즘

- Apriori 알고리즘 : 여러 단계를 반복적으로 실행하여 연관 규칙 생성
- FP-growth 알고리즘 : 빈출 패턴 성장, Apriori 알고리즘의 개선 버전

# 5. 이상 탐지 알고리즘 이해하기

- 이상(Anomaly) : 무언가 다르거나 비정상적이거나 특이하거나 잘 분류되지 않는 것
- 신용 카드 사기 범죄 탐지
- MRI 스캔을 이용한 악성 종양 탐지
- 네트워크 클러스터의 시스템 장애 탐지 및 대응
- 고속도로상 교통 사고 탐지

## 1. 클러스터링 알고리즘

- 비슷한 데이터 포인트끼리 묶을 수 있음

## 2. 밀도 기반 이상 탐지 알고리즘

- 가까운 이웃을 찾아냄
- K-최근접 이웃(K-Nearest Neighbors, KNN) 알고리즘

## 3. 서포트 벡터 머신 알고리즘

- 데이터 포인트의 경계 학습 가능
- 경계 밖에서 발견되는 데이터 포인트를 이상치로 식별