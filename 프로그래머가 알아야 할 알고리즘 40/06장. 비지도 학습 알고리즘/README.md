# 06장. 비지도 학습 알고리즘

# 1. 비지도 학습 이해하기

- 데이터에 내재된 패턴을 탐색하고 이를 이용해 비정형 데이터를 구조화하는 프로세스

## 1. 데이터 마이닝 사이클에서의 비지도 학습

- CRISP-DM(Cross-Industry Standard Process for Data Mining) 라이프 사이클
    - 비즈니스 이해 : 비즈니스 관점에서 문제와 요구사항을 세밀하게 이해
    - 데이터 이해 : 데이터 마이닝에 사용할 데이터 이해
    - 데이터 준비 : 훈련할 머신러닝 모델에 필요한 데이터 준비
    - 모델링 : 앞서 발견한 패턴을 활용해서 지도 학습 수행
    - 평가 : 훈련이 끝난 모델의 성능을 3단계에서 만든 테스트 데이터로 평가
    - 배포 : 모델을 프로덕션 환경에 배포하여 1단계에서 정의한 문제의 솔루션 제공
- SEMMA(Sample, Explore, Modify, Model, Access) 데이터 마이닝 프로세스

## 2. 비지도 학습의 최신 연구 트렌드

- 지도하지 않기 때문에 가정에 덜 의존적
- 어떤 차원에서든 잠재적으로 수렴할 수 있음

## 3. 비지도 학습의 활용 사례

- 마케팅 세분화
- 사기 탐지
- 장바구니 분석
- 음성 분류
- 문서 분류

# 2. 클러스터링 알고리즘 이해하기

## 1. 유사도 측정하기

### 1. 유클리드 거리(Euclidean)

- 다차원 공간에 위치한 두 포인트 사이의 가장 짧은 거리

### 2. 맨해튼 거리(Manhatten)

- 두 포인트 사이의 가장 긴 경로
- 맨해튼 거리 ≥ 유클리드 거리

### 3. 코사인 거리

- 원점에 연결된 두 포인트가 만들어내는 각도의 코사인 값을 계산하여 구함

### 4. k-평균 클러스터링 알고리즘

- 평균을 이용해 데이터 포인트 간 거리 계산, k개의 클러스터 생성
- 클러스터의 중심점이 클러스터에 속하는 데이터 포인트를 가장 잘 대표할 때까지 중심점을 옮기는 과정 반복

### 5. k-평균 클러스터링 알고리즘 로직

- 초기 설정 : 거리 측정 방식 결정
- 클러스터 개수 k를 정함
- 데이터 포인트 중에서 k개를 골라 클러스터 중심점을 설정
- 선택한 거리 측정 방식을 이용해 문제 공간상의 각 데이터 포인트와 k개의 클러스터 중심점 사이의 거리를 반복적으로 계산
- 문제 공간상의 각 데이터 포인트를 가장 가까운 클러스터 중심점에 할당
- 현재 설정된 클러스터 중심점이 실제로 각 클러스터의 무게 중심인지 확인
- 종료 조건 : 클러스터 중심점이 더 이상 변화하지 않을 때
    - 최대 실행 시간 설정
    - 최대 반복 횟수 설정

### 6. k-평균 클러스터링 알고리즘 코딩하기

```python
from sklearn import cluster
import pandas as pd
import numpy as np
import matplotlib.plot as plt

dataset = pd.DataFrame({'x': [11, 21, 28, 17, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 62, 70, 72, 10],
'y':[39, 36, 30, 52, 53, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 18, 7, 24, 10]})

myKmeans = cluster.KMeans(n_clusters=2)
myKmeans.fit(dataset)

labels = myKmeans.labels_
centers = myKmeans.cluster_centers_

print(labels)
print(centers)

plt.scatter(dataset['x'], dataset['y'], s=10)
plt.scatter(centers[0], centers[1], s=100)
plt.show()
```

### 7. k-평균 클러스터링 알고리즘의 한계

- 클러스터 개수를 미리 설정해야 함
- 클러스터 중심점을 초기에 무작위로 설정 → 알고리즘을 실행할 때마다 클러스터링 결과 달라짐
- 각 데이터 포인트를 오직 하나의 클러스터에만 할당
- 이상치에 취약

## 2. 계층적 클러스터링 알고리즘

- 상향(Bottom-Up) 방식 알고리즘
- 비슷한 데이터 포인트끼리 묶어서 점진적으로 클러스터 중심점으로 이동

### 1. 계층적 클러스터링 알고리즘의 단계

- 문제 공간에 있는 각 데이터 포인트마다 클러스터 생성
- 서로 가장 가까이 위치한 포인트끼리 묶음
- 종료 조건 확인
- 덴드로그램(Dendrogram) : 알고리즘을 통해 얻는 클러스터 구조

### 2. 계층적 클러스터링 알고리즘 코딩하기

```python
from sklean.cluster import AgglomerativeClustering
import pandas as pd
import numpy as np

dataset = pd.DataFrame({'x': [11, 21, 28, 17, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 62, 70, 72, 10],
'y':[39, 36, 30, 52, 53, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 18, 7, 24, 10]})

cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
cluster.fit_predict(dataset)

print(cluster.labels_)
```

## 3. 클러스터 평가하기

- 같은 클러스터에 할당된 데이터 포인트들은 가능한 서로 비슷해야 함
- 다른 클러스터에 할당된 데이터 포인트들은 가능한 서로 달라야 함
- 실루엣 분석(Silhouette Analysis) : 클러스터가 얼마나 뭉쳐 있고 분리되어 있는지 평가하는 기법
    - 0.71 ~ 1.00 : 상, 클러스터들이 서로 상당히 분리되어 있음
    - 0.51 ~ 0.70 : 중 : 클러스터들이 서로 어느 정도 분리되어 있음
    - 0.26 ~ 0.50 : 하, 클러스터가 만들어지기는 했으나 그 결과를 신뢰하기 어려움
    - < 0.25 : 클러스터가 발견되지 않음, 입력한 파라미터와 데이터를 이용해 클러스터링하는 데 실패

## 4. 클러스터링 활용 사례

- 범죄 다발 지역 분석
- 인구 통계 및 사회 분석
- 시장 세분화
- 타게팅 광고
- 고객 카테고리 분류
- 주성분 분석(Principal Component Analysis, PCA)