# 08장. 뉴럴 네트워크 알고리즘

# 1. 뉴럴 네트워크(Artificial Neural Network, ANN) 이해하기

- 인간의 두뇌 속 뉴런에 영감
- 퍼셉트론(Perceptron) : 은닉층이 없는 간단한 뉴럴 네트워크, 선형 회귀 모델과 비슷
    - 각 입력 신호에 해당하는 가중치를 곱한 후 모두 더함
    - 가중합의 결과에 따라 참 또는 거짓을 출력하는 이진 분류기
    - 탐색하려는 신호가 입력 데이터에서 발견되면 퍼셉트론은 참 신호 발생

# 2. 뉴럴 네트워크의 발전 살펴보기

- 퍼셉트론은 복잡한 로직을 학습할 수 없음
- XOR과 같은 단순한 로직 함수도 학습하기 어려움
- AI 겨울 : 뉴럴 네트워크에 대한 세간의 관짐 떨어짐, 암흑기
- AI 봄 : 분산 컴퓨팅 기술 발전 및 인프라 보급
- 다층 퍼셉트론(Multilayer Perceptron) : 여러 층으로 구성된 뉴럴 네트워크
    - 입력층(Input Layer)
    - 은닉층(Hidden Layer)
    - 출력층(Output Layer)
- 뉴런 : 네트워크를 구성하는 기본 단위, 한 층에 속한 뉴런들은 다음 층의 모든 뉴런들에 연결

# 3. 뉴럴 네트워크 훈련하기

- 연산을 통해 최적의 가중치 값을 구하는 것

## 1. 뉴럴 네트워크 구조

- 층(Layer) : 뉴럴 네트워크를 구성하는 핵심 요소
    - 하나 이상의 입력 데이터를 어떤 형태로 가공하여 하나 이상의 출력 데이터를 만들어냄
    - 데이터가 층 하나를 통과할 때마다 뉴럴 네트워크는 질문과 연관된 패턴을 찾으려 노력
- 손실 함수(Loss Function) : 학습 프로세스에서 피드백 신호 제공, 하나의 샘플이 정답에서 얼마나 멀어졌는지 알려 주는 역할 수행
- 비용 함수(Cost Function) : 전체 샘플에 대한 손실 함수
- 옵티마이저(Optimizer) : 손실 함수에서 얻은 피드백 시그널을 어떻게 해석할지 결정
- 입력 데이터(Input Data) : 뉴럴 네트워크를 훈련하는 데 쓰는 데이터
- 가중치(Weight) : 각 입력 정보를 그 중요성에 맞게 조절하는 역할
- 활성화 함수(Activation Fuction) : 입력 정보를 가중치와 곱하고 나서 집계하는 것

## 2. 경사하강법

- 초기에 무작위로 설정된 가중치를 반복 수정하여 최적화
- 가중치는 비용을 최소화하는 방향으로 수정
    - 방향(Direction) : 손실 함수의 최저점에 도달하기 위해 어떤 방향으로 이동할 것인가?
    - 학습률(Learning Rate) : 선택한 방향으로 얼마나 멀리 이동할 것인가?
- 역전파(Backpropagation) : 마지막 층의 기울기를 먼저 계산하고 그 다음 층 처리

## 3. 활성화 함수

- 뉴런에 전달된 입력 정보를 처리해 출력하는 방식
- 적절한 활성화 함수 선택 중요

### 1. 임계 함수(Threshold Function)

- 이진, 0 또는 1
- 활성화 함수의 입력값이 0보다 크면 1 출력

### 2. 시그모이드 함수(Sigmoid Fuction)

- 임계 함수의 단점 보완
- 활성화 함수의 민감도 조절 가능
- 0과 1 사이의 실수로 출력

### 3. ReLU 함수

- 입력 변수를 연속적인 단일 출력값으로 변환
- 변수의 연속성을 유지하려는 은닉층에 사용
- 입력 정보 중 0보다 작은 값은 모두 0으로 변환

### 4. Leaky ReLU 함수

- ReLU 함수의 단점 보완
- ß 값 임의 설정
- ß 값 뉴럴 네트워크의 파라미터로 설정하여 훈련을 통해 적절한 값 학습, 파라메트릭 ReLU(Parametric ReLU)
- ß 값 무작위 설정, 무작위 ReLU(Randomized ReLU)

### 5. Tanh 함수

- 시그모이드 함수와 비슷
- 신호를 음수로 출력 가능

### 6. 소프트맥스 함수

- 다중 클래스 분류 문제에 적합
- 각 클래스별 예측 확률 출력